{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.5 with Spark",
      "name": "python3",
      "language": "python3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.5.4",
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "Capstone_PredictLiverFailure.model_def_train_eval.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "q3GWXCUaia4e"
      },
      "source": [
        "# Predict Liver Failure based on People's Demographics\n",
        "\n",
        "***(Model Definition, Training and Evaluation Notebook)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot5uCAQpia4g"
      },
      "source": [
        "## 4. Model Definition, Training and Evaluation\n",
        "\n",
        "In this notebook, we will select the model which is most appropriate for our usecase i.e. the model that is most appropriate for accurately predicting the possibility of a Liver Failure in individuals based on that demographics and health information data that we have that was gathered from the JPAC Center for Health Diagnosis and Control."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8FmHmbCia4h"
      },
      "source": [
        "### 4.1. Load Feature Engineered Data from Data Store\n",
        "\n",
        "Let us start by loading the data from the IBM Data store onto this notebook for further processing. Now we will to connect to the object store and read a PARQUET file and create a dataframe out of it. Using SparkSQL we can handle it like a database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMyPNGZvia4i",
        "outputId": "4b80211e-cd0e-4626-9bbe-44274c8290e2"
      },
      "source": [
        "# import required packages and libraries\n",
        "import types\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ibmos2spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Waiting for a Spark session to start...\n",
            "Spark Initialization Done! ApplicationId = app-20190405085903-0002\n",
            "KERNEL_ID = f22aa495-7cf8-4d49-8bdd-9402d6b8ebca\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1AaPabqia4j",
        "outputId": "76dee740-f0d4-4b8d-ebbf-0906ad94f814"
      },
      "source": [
        "credentials = {\n",
        "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
        "    'api_key': 'yR6pr44dLxKcEe_-J-YBRKtI9LaoOcG9v_c2zK_I1epP',\n",
        "    'service_id': 'iam-ServiceId-dd08a5f3-28d2-4f87-bc12-4ec0662689f2',\n",
        "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token'}\n",
        "\n",
        "configuration_name = 'os_85bf8a7fa4e54387abd3bbb49b9490af_configs'\n",
        "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "df_data = spark.read.parquet(cos.url('ALF_Normalized.parquet', 'fundamentalsofscalabledatascience-donotdelete-pr-qbkdskud4vsck0'))\n",
        "print(\"Number of records = \", df_data.count(), \"\\n\")\n",
        "df_data.createOrReplaceTempView('alf_data')\n",
        "df_data.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of records =  5221 \n",
            "\n",
            "+---+--------------------+\n",
            "|ALF|            features|\n",
            "+---+--------------------+\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,0.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,0.0,1.0,1.0,...|\n",
            "|0.0|[0.5,0.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,0.0,1.0,1.0,...|\n",
            "|0.0|[0.5,0.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|1.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,0.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,0.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|0.0|[0.5,1.0,1.0,1.0,...|\n",
            "+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsK5ua5hia4j"
      },
      "source": [
        "Let us rename the column ALF to label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QwrRm4Tia4k",
        "outputId": "54839e2f-c48a-433c-8959-99d1b8e13d23"
      },
      "source": [
        "df_data_new = df_data.withColumnRenamed('ALF', 'label')\n",
        "df_data_new.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,0.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,0.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,0.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,0.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,0.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  1.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,0.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,0.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "|  0.0|[0.5,1.0,1.0,1.0,...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y19wTed9ia4m"
      },
      "source": [
        "Now let us split the data into training and test datasets - 80% training data and 20% test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKRjDYxTia4n"
      },
      "source": [
        "splits = df_data_new.randomSplit([0.8, 0.2])\n",
        "df_train = splits[0] # training dataset\n",
        "df_test = splits[1] # test dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXkc3bi4ia4o"
      },
      "source": [
        "Let us take a quick look at how the class label is split between the training and test data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JknmEIt5ia4o",
        "outputId": "c97d6177-bbbd-4757-8b43-2adc596fb5d4"
      },
      "source": [
        "df_train.createOrReplaceTempView('df_train')\n",
        "spark.sql(\"select label, count(*) from df_train group by label\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------+\n",
            "|label|count(1)|\n",
            "+-----+--------+\n",
            "|  0.0|    3915|\n",
            "|  1.0|     281|\n",
            "+-----+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foG_1SG6ia4p",
        "outputId": "daa36fe4-48dd-4ced-abd6-5a9b1c7d9b2d"
      },
      "source": [
        "df_test.createOrReplaceTempView('df_test')\n",
        "spark.sql(\"select label, count(*) from df_test group by label\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------+\n",
            "|label|count(1)|\n",
            "+-----+--------+\n",
            "|  0.0|     952|\n",
            "|  1.0|      73|\n",
            "+-----+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBYpzp7yia4p"
      },
      "source": [
        "### 4.2. Choice of Model\n",
        "\n",
        "In our usecase we are trying to predict the possibility of an individual running into a Liver failure. For modeling this usecase, we have a dataset generated by JPAC Center for Health Diagnosis and Control in which we have a bunch of features that can be used for our prediction and we also have a target variable or label which gives us a binary value of 0 or 1 which would tell us the possiblity of a liver failure.\n",
        "\n",
        "Considering our usecase and the dataset, ours is a case of Supervised Machine Learning and can precisely be categorised as a Binary Classification Model.\n",
        "\n",
        "#### 4.2.1 Choice of Machine Learning Algorithm\n",
        "\n",
        "Now, we have figured out that we should be using a Supervised Machine Learning Algorithm for defining our model. The next task is to figure out the Supervised Machine Learning Algorithm that would best suit our usecase and dataset for predicting possiblity of Liver Failures in individuals.\n",
        "\n",
        "    1. Linear Regression - This algorithm is used to predict a continuous value.\n",
        "    2. Logistic Regression - This algorithm is used to predict a binary classifier instead of a continuous variable.\n",
        "    3. Naive Bayes - This is a classification algorithm for binary (two-class) and multi-class classification problems.\n",
        "    4. Support Vector Machine - This is a binary classifier that analyze data used for classification and regression analysis.\n",
        "    5. Gradient Boosted Trees - This can also be used for regression and classification problems.\n",
        "\n",
        "We can see that we have more than one algorithms to choose from. For our use case, we will go ahead and use ***Gradient Boosted Trees*** because Gradient Boosting is one of the more powerful techniques for building predictive models.\n",
        "\n",
        "#### 4.2.2 Choice of Deep Learning Algorithm\n",
        "\n",
        "With respect to Deep Learning Algorithms, there is a wide range of algorithms to choose from. Based on our usecase and dataset, we will use a ***Feed Forward Neural Network / Multi Layer Perceptron*** for our model as the Perceptron is a binary linear classifier and meets our needs for this use case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdGQPTUGia4q"
      },
      "source": [
        "### 4.3. Gradient Boosted Trees - Supervised Machine Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSL2eoJdia4q"
      },
      "source": [
        "#### 4.3.1. Data Prep for Model\n",
        "\n",
        "The data prep that we have done so far addresses the data needs for this model. Earlier in this notebook, we have also split the data into training and test datasets. This will be used for training and evaluating our model performance and for measuring the training and validation accuracy for our Supervised Machine Learning Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDGr9DKTia4q"
      },
      "source": [
        "#### 4.3.2. Model Definition and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg8xwurZia4r"
      },
      "source": [
        "Let us now define our Supervised Machine Learning Model using the Gradient Boosted Trees Algorithm and train it using the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9pKwJNwia4r"
      },
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbt = GBTClassifier(labelCol='label', featuresCol='features', maxIter=20)\n",
        "\n",
        "model = gbt.fit(df_train)\n",
        "prediction = model.transform(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOYs252Fia4r"
      },
      "source": [
        "#### 4.3.3. Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUV_B9Dzia4r"
      },
      "source": [
        "We will capture ***accuracy*** as a measure of evaluation for our model.\n",
        "\n",
        "Let us now validate the training performance of our model against our training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohVbDxCLia4r",
        "outputId": "035444a9-3688-4ae9-ccf5-a3958620b61f"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator().setMetricName('accuracy').setPredictionCol('prediction').setLabelCol('label')\n",
        "print(\"GBT Training Accuracy\")\n",
        "print(\"---------------------\")\n",
        "evaluator.evaluate(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GBT Training Accuracy\n",
            "---------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9611534795042898"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIm2g02ia4s"
      },
      "source": [
        "The performance of our model against our training dataset appears to be pretty good at 0.96 (96%). \n",
        "\n",
        "Now let us validate the performance of our model using test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZmkYHaKia4s",
        "outputId": "f74df906-4297-49ca-98a4-ea7ba86df3d8"
      },
      "source": [
        "model_test = gbt.fit(df_test)\n",
        "prediction_test = model.transform(df_test)\n",
        "print(\"GBT Validation Accuracy\")\n",
        "print(\"---------------------\")\n",
        "evaluator.evaluate(prediction_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GBT Validation Accuracy\n",
            "---------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9219512195121952"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsnZQDqqia4s"
      },
      "source": [
        "The performance of our model against our test / validation dataset also appears to be pretty good at 0.92 (92%).  \n",
        "\n",
        "Overall, based on the above results, we can say that our Gradient Boosted Trees model has done a pretty good job of predicting  the possibility of Liver failure based on our feature set. The accuracy of our model with both the training and test datasets is as follows:  \n",
        "* Training Accuracy = 0.96 (96%)\n",
        "* Validation Accuracy = 0.92 (92%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKSHJTWUia4s"
      },
      "source": [
        "### 4.4. Feed Forward Neural Network (MLP) - Deep Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD7UYKt7ia4s"
      },
      "source": [
        "#### 4.4.1. Data Prep for Model\n",
        "\n",
        "In our data prep so far, we have created training and test datasets to be used for training and testing our model. However this data is represented as dataframes. Our Feed Forward Neural Network model here is implemented using Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqI3hIAaia4t"
      },
      "source": [
        "This Feed Forward Neural Network model expects the input and output dataset as arrays. So, let us first construct input and output data arrays from our training and test datasets.  \n",
        "\n",
        "Let us first define a function which will take a dataframe as input and return the input array (Features array) and output array (Label array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGc5Qcwuia4t"
      },
      "source": [
        "#########################################################\n",
        "# FUNCTION TO CONSTRUCT INPUT AND OUTPUT DATASET ARRAYS #\n",
        "########################################################\n",
        "def construct_arrays (df):\n",
        "    # Initialize Input and Output arrays for the Data set\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    # Convert dataframe from Spark DF to Pandas DF\n",
        "    df_pd = df.toPandas()\n",
        "\n",
        "    # Loop through dataframe and add data to input and output arrays\n",
        "    for index, row in df_pd.iterrows():\n",
        "        X.append(row[1])\n",
        "        y.append(row[0])\n",
        "\n",
        "    # Convery input and output data arrays from Python arrays to Numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    return (X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0RNE3rpia4t"
      },
      "source": [
        "Now let us pass the training and test data frames to the above function to obtain the respective input and output arrays containing the feature set and the label data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN4R5Yx7ia4t",
        "outputId": "f47dbaeb-db6e-4ee6-db45-b29f9ceca58a"
      },
      "source": [
        "X_train, y_train = construct_arrays(df_train)\n",
        "print(\"Size of Training Features dataset: \", len(X_train))\n",
        "print(X_train)\n",
        "print(\"Size of Training Label dataset: \", len(y_train))\n",
        "print(y_train)\n",
        "\n",
        "X_test, y_test = construct_arrays (df_test)\n",
        "print(\"Size of Test Features dataset: \", len(X_test))\n",
        "print(X_test)\n",
        "print(\"Size of Test Label dataset: \", len(y_test))\n",
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Training Features dataset:  4196\n",
            "[[ 0.5         0.          0.         ...,  0.19736842  0.32068311\n",
            "   0.31073446]\n",
            " [ 0.5         0.          0.         ...,  0.51973684  0.33017078\n",
            "   0.41242938]\n",
            " [ 0.5         0.          0.         ...,  0.18421053  0.33965844\n",
            "   0.32580038]\n",
            " ..., \n",
            " [ 0.5         1.          1.         ...,  0.32236842  0.21442125\n",
            "   0.24105461]\n",
            " [ 0.5         1.          1.         ...,  0.20394737  0.13851992\n",
            "   0.13182674]\n",
            " [ 0.5         1.          1.         ...,  0.42763158  0.21252372\n",
            "   0.2693032 ]]\n",
            "Size of Training Label dataset:  4196\n",
            "[ 0.  0.  0. ...,  1.  1.  1.]\n",
            "Size of Test Features dataset:  1025\n",
            "[[ 0.5         0.          0.         ...,  0.20394737  0.28462998\n",
            "   0.27683616]\n",
            " [ 0.5         0.          0.         ...,  0.28289474  0.4573055\n",
            "   0.47080979]\n",
            " [ 0.5         0.          0.         ...,  0.17105263  0.32637571\n",
            "   0.30885122]\n",
            " ..., \n",
            " [ 0.5         1.          1.         ...,  0.35526316  0.27514231\n",
            "   0.31073446]\n",
            " [ 0.5         1.          1.         ...,  0.57236842  0.16129032\n",
            "   0.25988701]\n",
            " [ 0.5         1.          1.         ...,  0.17763158  0.28083491\n",
            "   0.26553672]]\n",
            "Size of Test Label dataset:  1025\n",
            "[ 0.  0.  0. ...,  1.  1.  1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj0pmuKFia4t"
      },
      "source": [
        "#### 4.4.2. Model Definition, Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM_CE-2xia4t"
      },
      "source": [
        "Let us now define our Deep Learning Model using the Feed Forward Neural Network (Multilayer Perceptron) Algorithm and then train and evaluate the model using the training and test / validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOcGWx5pia4t",
        "outputId": "7300a173-b83c-44de-e7a7-1259f63ebd57"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Define a Multilayer Perceptron (MLP) Model using Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim = 25, kernel_initializer = 'normal', activation = 'relu')) # input Layer\n",
        "model.add(Dense(1, kernel_initializer = 'normal', activation = 'sigmoid')) # Output Layer\n",
        "\n",
        "# Compile our model\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adadelta', metrics = ['accuracy'])\n",
        "\n",
        "# Train our model\n",
        "model.fit(X_train, y_train, epochs = 20, batch_size = 5, verbose = 1, validation_data = (X_test, y_test))\n",
        "\n",
        "# Evaluate our model\n",
        "score = model.evaluate(X_test, y_test, verbose = 0)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 4196 samples, validate on 1025 samples\n",
            "Epoch 1/20\n",
            "4196/4196 [==============================] - 2s 370us/step - loss: 0.2299 - acc: 0.9330 - val_loss: 0.2136 - val_acc: 0.9288\n",
            "Epoch 2/20\n",
            "4196/4196 [==============================] - 1s 298us/step - loss: 0.1968 - acc: 0.9333 - val_loss: 0.1970 - val_acc: 0.9288\n",
            "Epoch 3/20\n",
            "4196/4196 [==============================] - 1s 297us/step - loss: 0.1894 - acc: 0.9335 - val_loss: 0.2020 - val_acc: 0.9288\n",
            "Epoch 4/20\n",
            "4196/4196 [==============================] - 1s 297us/step - loss: 0.1863 - acc: 0.9335 - val_loss: 0.1862 - val_acc: 0.9317\n",
            "Epoch 5/20\n",
            "4196/4196 [==============================] - 1s 322us/step - loss: 0.1836 - acc: 0.9330 - val_loss: 0.1851 - val_acc: 0.9317\n",
            "Epoch 6/20\n",
            "4196/4196 [==============================] - 1s 328us/step - loss: 0.1817 - acc: 0.9328 - val_loss: 0.1833 - val_acc: 0.9317\n",
            "Epoch 7/20\n",
            "4196/4196 [==============================] - 1s 304us/step - loss: 0.1803 - acc: 0.9330 - val_loss: 0.1813 - val_acc: 0.9298\n",
            "Epoch 8/20\n",
            "4196/4196 [==============================] - 1s 297us/step - loss: 0.1787 - acc: 0.9326 - val_loss: 0.1835 - val_acc: 0.9298\n",
            "Epoch 9/20\n",
            "4196/4196 [==============================] - 1s 303us/step - loss: 0.1774 - acc: 0.9337 - val_loss: 0.1799 - val_acc: 0.9307\n",
            "Epoch 10/20\n",
            "4196/4196 [==============================] - 1s 297us/step - loss: 0.1769 - acc: 0.9352 - val_loss: 0.1788 - val_acc: 0.9298\n",
            "Epoch 11/20\n",
            "4196/4196 [==============================] - 1s 301us/step - loss: 0.1764 - acc: 0.9326 - val_loss: 0.1841 - val_acc: 0.9307\n",
            "Epoch 12/20\n",
            "4196/4196 [==============================] - 1s 309us/step - loss: 0.1749 - acc: 0.9335 - val_loss: 0.1804 - val_acc: 0.9288\n",
            "Epoch 13/20\n",
            "4196/4196 [==============================] - 1s 300us/step - loss: 0.1745 - acc: 0.9326 - val_loss: 0.1809 - val_acc: 0.9327\n",
            "Epoch 14/20\n",
            "4196/4196 [==============================] - 1s 306us/step - loss: 0.1748 - acc: 0.9333 - val_loss: 0.1858 - val_acc: 0.9307\n",
            "Epoch 15/20\n",
            "4196/4196 [==============================] - 1s 309us/step - loss: 0.1742 - acc: 0.9321 - val_loss: 0.1865 - val_acc: 0.9307\n",
            "Epoch 16/20\n",
            "4196/4196 [==============================] - 1s 306us/step - loss: 0.1731 - acc: 0.9330 - val_loss: 0.1809 - val_acc: 0.9307\n",
            "Epoch 17/20\n",
            "4196/4196 [==============================] - 1s 291us/step - loss: 0.1742 - acc: 0.9337 - val_loss: 0.1845 - val_acc: 0.9307\n",
            "Epoch 18/20\n",
            "4196/4196 [==============================] - 1s 293us/step - loss: 0.1740 - acc: 0.9352 - val_loss: 0.1778 - val_acc: 0.9288\n",
            "Epoch 19/20\n",
            "4196/4196 [==============================] - 1s 296us/step - loss: 0.1738 - acc: 0.9347 - val_loss: 0.1783 - val_acc: 0.9288\n",
            "Epoch 20/20\n",
            "4196/4196 [==============================] - 1s 301us/step - loss: 0.1727 - acc: 0.9354 - val_loss: 0.1812 - val_acc: 0.9307\n",
            "[0.18123500248280966, 0.93073170731707322]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXUR3CdUia4u"
      },
      "source": [
        "We can see that the training and validation accuracy of our Feed Forward Neural Network is also pretty good. The accuracy values as measured for our model is as follows:\n",
        "* Training accuracy = 0.94 (94%)\n",
        "* Validation accuracy = 0.93 (93%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoK93k1iia4u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}